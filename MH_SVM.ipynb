{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanetteFu/Message_Detect/blob/main/MH_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUtSh42sqyGD"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaS4sBkAq7uk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdb66e98-c171-48f1-e367-09eed9274243"
      },
      "source": [
        "import pandas as pd\n",
        "tweets = pd.read_csv(\"clean_tweet_hatespeech.csv\")\n",
        "\n",
        "\n",
        "tweets['label'] = tweets['label'].replace(1,4)\n",
        "tweets['label'] = tweets['label'].replace(0,1)\n",
        "tweets['label'] = tweets['label'].replace(4,0)\n",
        "list(tweets.columns.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Unnamed: 0', 'text', 'hashtags', 'emojis', 'label']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExrL59dzrFwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4ee9e3d6-c91b-4caf-ebce-980553ea77be"
      },
      "source": [
        "tweets = tweets.sample(frac=1)\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>emojis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27216</th>\n",
              "      <td>29275</td>\n",
              "      <td>what s happing in # orlando is really shocking we must remember that behind the death toll was someone son or daughter</td>\n",
              "      <td>['#orlando']</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5150</th>\n",
              "      <td>5534</td>\n",
              "      <td>be n # healthy every day do something that will inch you closer to a better tomorrow doug firebaugh</td>\n",
              "      <td>['#healthy']</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30109</th>\n",
              "      <td>5643</td>\n",
              "      <td>can not wait to see u your admin hang for # treason # trump # make america great again</td>\n",
              "      <td>['#treason', '#trump', '#makeamericagreatagain']</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>1037</td>\n",
              "      <td>like the spread of peanut butter on white bread # little wonders</td>\n",
              "      <td>['#littlewonders']</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21549</th>\n",
              "      <td>23162</td>\n",
              "      <td># cute</td>\n",
              "      <td>['#cute']</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ... label\n",
              "27216  29275       ...  1   \n",
              "5150   5534        ...  1   \n",
              "30109  5643        ...  0   \n",
              "960    1037        ...  1   \n",
              "21549  23162       ...  1   \n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hKqhyPtrHpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7c88bdc7-2c3a-4529-ede2-94472157b2de"
      },
      "source": [
        "label_counts = tweets.label.value_counts()\n",
        "print(label_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    29720\n",
            "0    2242 \n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CppKeMnEWFTC"
      },
      "source": [
        "stop_words = pd.read_csv(\"stopwords2.csv\")\n",
        "stop_words = stop_words['0'].tolist()\n",
        "stop_words = set(stop_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shUp3ViorU1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f5227f0b-56d0-49f5-a6ce-49bb3552db53"
      },
      "source": [
        "import re, nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "#from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "def normalizer(tweet):\n",
        "  tokens = nltk.word_tokenize(tweet)\n",
        "  filtered_result = list(filter(lambda l: l not in stop_words, tokens))\n",
        "  lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "  return lemmas\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTHDqkY2r6av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "96e949c3-8dee-4ade-a2f0-b171f6711b5d"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "tweets['text'] = tweets.text.apply(str)\n",
        "tweets['normalized_tweet'] = tweets.text.apply(normalizer)\n",
        "tweets[['text', 'normalized_tweet']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>normalized_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27216</th>\n",
              "      <td>what s happing in # orlando is really shocking we must remember that behind the death toll was someone son or daughter</td>\n",
              "      <td>[s, happing, #, orlando, shocking, must, remember, behind, death, toll, someone, son, or, daughter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5150</th>\n",
              "      <td>be n # healthy every day do something that will inch you closer to a better tomorrow doug firebaugh</td>\n",
              "      <td>[n, #, healthy, every, something, inch, closer, a, better, tomorrow, doug, firebaugh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30109</th>\n",
              "      <td>can not wait to see u your admin hang for # treason # trump # make america great again</td>\n",
              "      <td>[wait, u, admin, hang, #, treason, #, trump, #, make, america, great, again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>like the spread of peanut butter on white bread # little wonders</td>\n",
              "      <td>[spread, peanut, butter, white, bread, #, little, wonder]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21549</th>\n",
              "      <td># cute</td>\n",
              "      <td>[#, cute]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                         text                                                                                     normalized_tweet\n",
              "27216  what s happing in # orlando is really shocking we must remember that behind the death toll was someone son or daughter  [s, happing, #, orlando, shocking, must, remember, behind, death, toll, someone, son, or, daughter]\n",
              "5150   be n # healthy every day do something that will inch you closer to a better tomorrow doug firebaugh                     [n, #, healthy, every, something, inch, closer, a, better, tomorrow, doug, firebaugh]              \n",
              "30109  can not wait to see u your admin hang for # treason # trump # make america great again                                  [wait, u, admin, hang, #, treason, #, trump, #, make, america, great, again]                       \n",
              "960    like the spread of peanut butter on white bread # little wonders                                                        [spread, peanut, butter, white, bread, #, little, wonder]                                          \n",
              "21549  # cute                                                                                                                  [#, cute]                                                                                          "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkLm1u-QsqTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e86f1d00-abe3-426d-e2c1-8ff99c571eba"
      },
      "source": [
        "from nltk import ngrams\n",
        "def ngrams(input_list):\n",
        "  onegrams = input_list #try with unigrams\n",
        "  bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
        "  trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
        "  return onegrams + bigrams + trigrams\n",
        "\n",
        "tweets['grams'] = tweets.normalized_tweet.apply(ngrams)\n",
        "tweets[['grams']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27216</th>\n",
              "      <td>[s, happing, #, orlando, shocking, must, remember, behind, death, toll, someone, son, or, daughter, s happing, happing #, # orlando, orlando shocking, shocking must, must remember, remember behind, behind death, death toll, toll someone, someone son, son or, or daughter, s happing #, happing # orlando, # orlando shocking, orlando shocking must, shocking must remember, must remember behind, remember behind death, behind death toll, death toll someone, toll someone son, someone son or, son or daughter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5150</th>\n",
              "      <td>[n, #, healthy, every, something, inch, closer, a, better, tomorrow, doug, firebaugh, n #, # healthy, healthy every, every something, something inch, inch closer, closer a, a better, better tomorrow, tomorrow doug, doug firebaugh, n # healthy, # healthy every, healthy every something, every something inch, something inch closer, inch closer a, closer a better, a better tomorrow, better tomorrow doug, tomorrow doug firebaugh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30109</th>\n",
              "      <td>[wait, u, admin, hang, #, treason, #, trump, #, make, america, great, again, wait u, u admin, admin hang, hang #, # treason, treason #, # trump, trump #, # make, make america, america great, great again, wait u admin, u admin hang, admin hang #, hang # treason, # treason #, treason # trump, # trump #, trump # make, # make america, make america great, america great again]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>[spread, peanut, butter, white, bread, #, little, wonder, spread peanut, peanut butter, butter white, white bread, bread #, # little, little wonder, spread peanut butter, peanut butter white, butter white bread, white bread #, bread # little, # little wonder]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21549</th>\n",
              "      <td>[#, cute, # cute]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           grams\n",
              "27216  [s, happing, #, orlando, shocking, must, remember, behind, death, toll, someone, son, or, daughter, s happing, happing #, # orlando, orlando shocking, shocking must, must remember, remember behind, behind death, death toll, toll someone, someone son, son or, or daughter, s happing #, happing # orlando, # orlando shocking, orlando shocking must, shocking must remember, must remember behind, remember behind death, behind death toll, death toll someone, toll someone son, someone son or, son or daughter]\n",
              "5150   [n, #, healthy, every, something, inch, closer, a, better, tomorrow, doug, firebaugh, n #, # healthy, healthy every, every something, something inch, inch closer, closer a, a better, better tomorrow, tomorrow doug, doug firebaugh, n # healthy, # healthy every, healthy every something, every something inch, something inch closer, inch closer a, closer a better, a better tomorrow, better tomorrow doug, tomorrow doug firebaugh]                                                                             \n",
              "30109  [wait, u, admin, hang, #, treason, #, trump, #, make, america, great, again, wait u, u admin, admin hang, hang #, # treason, treason #, # trump, trump #, # make, make america, america great, great again, wait u admin, u admin hang, admin hang #, hang # treason, # treason #, treason # trump, # trump #, trump # make, # make america, make america great, america great again]                                                                                                                                    \n",
              "960    [spread, peanut, butter, white, bread, #, little, wonder, spread peanut, peanut butter, butter white, white bread, bread #, # little, little wonder, spread peanut butter, peanut butter white, butter white bread, white bread #, bread # little, # little wonder]                                                                                                                                                                                                                                                      \n",
              "21549  [#, cute, # cute]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYVuw5PftEGl"
      },
      "source": [
        "import collections\n",
        "def count_words(input):\n",
        "  count = collections.Counter()\n",
        "  for row in input:\n",
        "    for word in row:\n",
        "      count[word] += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC6vPSeLtUtY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "5e897a36-a67d-43ca-efe3-d6881ae9aac9"
      },
      "source": [
        "tweets[(tweets.label == 0)][['grams']].apply(count_words)['grams'].most_common(50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('#', 4810),\n",
              " ('a', 679),\n",
              " ('s', 353),\n",
              " ('i', 292),\n",
              " ('trump', 280),\n",
              " ('white', 213),\n",
              " ('black', 190),\n",
              " ('# trump', 165),\n",
              " ('woman', 137),\n",
              " ('w', 134),\n",
              " ('re', 132),\n",
              " ('u', 130),\n",
              " ('by', 119),\n",
              " ('racist', 119),\n",
              " ('new', 110),\n",
              " ('people', 107),\n",
              " ('m', 105),\n",
              " ('racism', 99),\n",
              " ('politics', 97),\n",
              " ('why', 96),\n",
              " ('obama', 96),\n",
              " ('# politics', 95),\n",
              " ('who', 94),\n",
              " ('liberal', 93),\n",
              " ('allah', 92),\n",
              " ('soil', 92),\n",
              " ('# allah', 92),\n",
              " ('allah soil', 92),\n",
              " ('# allah soil', 92),\n",
              " ('hate', 89),\n",
              " ('a #', 88),\n",
              " ('his', 87),\n",
              " ('right', 86),\n",
              " ('# black', 84),\n",
              " ('tweet', 83),\n",
              " ('# liberal', 82),\n",
              " ('lib', 80),\n",
              " ('# lib', 79),\n",
              " ('feminism', 78),\n",
              " ('w #', 77),\n",
              " ('might', 77),\n",
              " ('tard', 77),\n",
              " ('lib tard', 77),\n",
              " ('# lib tard', 77),\n",
              " ('sj', 76),\n",
              " ('# sj', 76),\n",
              " ('sj w', 76),\n",
              " ('# sj w', 76),\n",
              " ('t', 75),\n",
              " ('liberal #', 75)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7j6poQPtgGp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "6ea4d105-d03a-4757-f50d-38465b1ca33b"
      },
      "source": [
        "tweets[(tweets.label == 1)][['grams']].apply(count_words)['grams'].most_common(50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('#', 71817),\n",
              " ('i', 7803),\n",
              " ('a', 6973),\n",
              " ('s', 3996),\n",
              " ('love', 3537),\n",
              " ('# #', 2145),\n",
              " ('# love', 2051),\n",
              " ('happy', 1853),\n",
              " ('life', 1814),\n",
              " ('u', 1804),\n",
              " ('love #', 1482),\n",
              " ('father', 1331),\n",
              " ('# love #', 1221),\n",
              " ('m', 1211),\n",
              " ('new', 1197),\n",
              " ('i m', 999),\n",
              " ('positive', 998),\n",
              " ('thankful', 960),\n",
              " ('# positive', 934),\n",
              " ('smile', 902),\n",
              " ('a #', 885),\n",
              " ('friend', 867),\n",
              " ('way', 861),\n",
              " ('our', 857),\n",
              " ('make', 854),\n",
              " ('girl', 840),\n",
              " ('people', 826),\n",
              " ('by', 775),\n",
              " ('life #', 756),\n",
              " ('fun', 730),\n",
              " ('family', 707),\n",
              " ('want', 699),\n",
              " ('best', 692),\n",
              " ('insta', 681),\n",
              " ('weekend', 678),\n",
              " ('# smile', 677),\n",
              " ('take', 673),\n",
              " ('orlando', 669),\n",
              " ('friday', 667),\n",
              " ('summer', 665),\n",
              " ('week', 654),\n",
              " ('healthy', 651),\n",
              " ('# i', 642),\n",
              " ('i #', 642),\n",
              " ('re', 642),\n",
              " ('# insta', 639),\n",
              " ('s #', 627),\n",
              " ('music', 618),\n",
              " ('# healthy', 614),\n",
              " ('who', 607)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU2PuCDqt-kX"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "\n",
        "import pickle\n",
        "pkl_filename = 'count_vectorizer.pkl'\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(count_vectorizer, file)\n",
        "\n",
        "\n",
        "# Load from file\n",
        "#with open('SVM_Hatespeech.pkl', 'rb') as file:\n",
        "    #pickle_model = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUgoxE__uJnU"
      },
      "source": [
        "vectorized_data = count_vectorizer.fit_transform(tweets.text)\n",
        "indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ4CP-5LuWGW"
      },
      "source": [
        "targets = tweets.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTYP_qAr0y6W"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\n",
        "data_train_index = data_train[:,0]\n",
        "data_train = data_train[:,1:]\n",
        "data_test_index = data_test[:,0]\n",
        "data_test = data_test[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o5d1iVR_o1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ed3998fe-8061-4bd8-be1b-f874fed96c0a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression(class_weight='balanced')\n",
        "logisticRegr.fit(data_train, targets_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TriKsN45_5T7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35d3f3c4-c75e-4a93-eb1a-975be5e0ddc4"
      },
      "source": [
        "#from sklearn.metrics import classification_report\n",
        "predictions = logisticRegr.predict(data_test)\n",
        "score = logisticRegr.score(data_test, targets_test)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9590926867422761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8uw-Q8L-s_c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "aaa842a4-dae8-4392-83f5-0deb1ca75476"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(targets_test, predictions))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.65      0.70       924\n",
            "           1       0.97      0.98      0.98     11861\n",
            "\n",
            "    accuracy                           0.96     12785\n",
            "   macro avg       0.86      0.81      0.84     12785\n",
            "weighted avg       0.96      0.96      0.96     12785\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITA9OZ4_vVzn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "522f609f-2fe8-4d26-a941-7bcc1f69812a"
      },
      "source": [
        "\n",
        "#import pickle\n",
        "\n",
        "pkl_filename = 'logisticRegrHateSpeech.pkl'\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(logisticRegr, file)\n",
        "\n",
        "# Load from file\n",
        "with open('logisticRegrHateSpeech.pkl', 'rb') as file:\n",
        "    pickle_model = pickle.load(file)\n",
        "# Calculate the accuracy score and predict target values\n",
        "score = pickle_model.score(data_test, targets_test)\n",
        "print('Test score: {0:.2f} %'.format(100 * score))\n",
        "Ypredict = pickle_model.predict(data_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 95.91 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEGW10_aJM9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e313cc30-fdf7-4009-ccc5-60593251c39e"
      },
      "source": [
        "mislabeled_logreg = []\n",
        "for input, prediction, label in zip(tweets.text, predictions, targets_test):\n",
        "  if prediction != label:\n",
        "    #print(input, 'has been classified as ', prediction, 'and should be ', label) \n",
        "    mislabeled_logreg.append(input)\n",
        "len(mislabeled_logreg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7B_S7Ia1TzH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58494371-6e01-43b0-e1f9-b3f145ef7388"
      },
      "source": [
        "\n",
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "clf = OneVsRestClassifier(svm.SVC(gamma=0.01, C=0.1, probability=True, class_weight='balanced', kernel='linear'))\n",
        "clf_output = clf.fit(data_train, targets_train)\n",
        "clf.score(data_test, targets_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.763046875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he7geICw_jU8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ab223e5a-bb3d-49b2-ea8a-774fadca3595"
      },
      "source": [
        "predictions = clf.predict(data_test)\n",
        "print(classification_report(targets_test, predictions))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.73      0.75      6390\n",
            "           1       0.75      0.80      0.77      6410\n",
            "\n",
            "    accuracy                           0.76     12800\n",
            "   macro avg       0.76      0.76      0.76     12800\n",
            "weighted avg       0.76      0.76      0.76     12800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFuCqqfi1mBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62d61ec6-3e55-48b6-92ba-a0d75e42ce50"
      },
      "source": [
        "'''\n",
        "pkl_filename = 'SVM_Hatespeech.pkl'\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(clf, file)\n",
        "'''\n",
        "\n",
        "# Load from file\n",
        "with open('SVM_Hatespeech.pkl', 'rb') as file:\n",
        "    pickle_model = pickle.load(file)\n",
        "\n",
        "# Calculate the accuracy score and predict target values\n",
        "score = pickle_model.score(data_test, targets_test)\n",
        "print('Test score: {0:.2f} %'.format(100 * score))\n",
        "Ypredict = pickle_model.predict(data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 86.73 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I2_FVafJoOO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fa9c240-ecd5-4355-cf8d-506e543146bc"
      },
      "source": [
        "mislabeled_svm = []\n",
        "for input, prediction, label in zip(tweets.text, predictions, targets_test):\n",
        "  if prediction != label:\n",
        "    #print(input, 'has been classified as ', prediction, 'and should be ', label) \n",
        "    mislabeled_svm.append(input)\n",
        "len(mislabeled_svm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3033"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_yT6ybjO6ur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7d1440d3-e7ef-45cd-819d-6ee3b504dc19"
      },
      "source": [
        "print(len(mislabeled_logreg))\n",
        "print(len(mislabeled_svm))\n",
        "print(len(set(mislabeled_logreg).intersection(mislabeled_svm)))\n",
        "print(len(set(mislabeled_logreg).difference(mislabeled_svm)))\n",
        "print(len(set(mislabeled_svm).difference(mislabeled_logreg)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2983\n",
            "3033\n",
            "2682\n",
            "272\n",
            "324\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}