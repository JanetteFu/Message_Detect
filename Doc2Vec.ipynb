{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRGts6eyVccKQlXn1I/vUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanetteFu/Message_Detect/blob/main/Doc2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th6F9do3n3OK"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/sample_data/clean_tweets_sentiment140_32k.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXPpEvSwn_cA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1ab8e50a-6d65-4649-9e6c-a740a5acdad0"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "data = shuffle(data)\n",
        "data = shuffle(data)\n",
        "data['sentiment'] = data['sentiment'].replace(4,'1')\n",
        "data['sentiment'] = data['sentiment'].replace(0,'0')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>emojis</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>328</td>\n",
              "      <td>not yet unfortunately another few weeks i ve been told how are you i ve been a little bit busy with my latest project</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>736</td>\n",
              "      <td>still doing my homework</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19841</th>\n",
              "      <td>1587841</td>\n",
              "      <td>you got an course like that in your degree never heard of it</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24409</th>\n",
              "      <td>1592409</td>\n",
              "      <td>you stupid girl hahah do not be negative i want it too but i have to wait we can be tortured together okay</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6543</th>\n",
              "      <td>6543</td>\n",
              "      <td>more snow this morning here in the adirondacks it s covering my crocus that were coming up</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ... sentiment\n",
              "328    328         ...  0       \n",
              "736    736         ...  0       \n",
              "19841  1587841     ...  1       \n",
              "24409  1592409     ...  1       \n",
              "6543   6543        ...  0       \n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCZI1ZZloFvQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1414d872-3bad-4d08-fdc5-074247e869b6"
      },
      "source": [
        "import re, nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "#from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "def normalizer(tweet):\n",
        "  tokens = nltk.word_tokenize(tweet)\n",
        "  #filtered_result = list(filter(lambda l: l not in stop_words, tokens))\n",
        "  lemmas = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
        "  return lemmas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr1jKcG1oHQ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "628f4301-e947-4941-c542-897be89b5881"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "data['text'] = data.text.apply(str)\n",
        "data['normalized_tweet'] = data.text.apply(normalizer)\n",
        "data[['text', 'normalized_tweet']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>normalized_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>not yet unfortunately another few weeks i ve been told how are you i ve been a little bit busy with my latest project</td>\n",
              "      <td>[not, yet, unfortunately, another, few, week, i, ve, been, told, how, are, you, i, ve, been, a, little, bit, busy, with, my, latest, project]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>still doing my homework</td>\n",
              "      <td>[still, doing, my, homework]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19841</th>\n",
              "      <td>you got an course like that in your degree never heard of it</td>\n",
              "      <td>[you, got, an, course, like, that, in, your, degree, never, heard, of, it]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24409</th>\n",
              "      <td>you stupid girl hahah do not be negative i want it too but i have to wait we can be tortured together okay</td>\n",
              "      <td>[you, stupid, girl, hahah, do, not, be, negative, i, want, it, too, but, i, have, to, wait, we, can, be, tortured, together, okay]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6543</th>\n",
              "      <td>more snow this morning here in the adirondacks it s covering my crocus that were coming up</td>\n",
              "      <td>[more, snow, this, morning, here, in, the, adirondacks, it, s, covering, my, crocus, that, were, coming, up]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                        text                                                                                                                               normalized_tweet\n",
              "328    not yet unfortunately another few weeks i ve been told how are you i ve been a little bit busy with my latest project  [not, yet, unfortunately, another, few, week, i, ve, been, told, how, are, you, i, ve, been, a, little, bit, busy, with, my, latest, project]\n",
              "736    still doing my homework                                                                                                [still, doing, my, homework]                                                                                                                 \n",
              "19841  you got an course like that in your degree never heard of it                                                           [you, got, an, course, like, that, in, your, degree, never, heard, of, it]                                                                   \n",
              "24409  you stupid girl hahah do not be negative i want it too but i have to wait we can be tortured together okay             [you, stupid, girl, hahah, do, not, be, negative, i, want, it, too, but, i, have, to, wait, we, can, be, tortured, together, okay]           \n",
              "6543   more snow this morning here in the adirondacks it s covering my crocus that were coming up                             [more, snow, this, morning, here, in, the, adirondacks, it, s, covering, my, crocus, that, were, coming, up]                                 "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si5qHwvO4I-x"
      },
      "source": [
        "data = data.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDpGDyqm4OJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "522b3117-334c-40b2-b731-64b9ff838965"
      },
      "source": [
        "data['normalized_tweet'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not',\n",
              " 'yet',\n",
              " 'unfortunately',\n",
              " 'another',\n",
              " 'few',\n",
              " 'week',\n",
              " 'i',\n",
              " 've',\n",
              " 'been',\n",
              " 'told',\n",
              " 'how',\n",
              " 'are',\n",
              " 'you',\n",
              " 'i',\n",
              " 've',\n",
              " 'been',\n",
              " 'a',\n",
              " 'little',\n",
              " 'bit',\n",
              " 'busy',\n",
              " 'with',\n",
              " 'my',\n",
              " 'latest',\n",
              " 'project']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvj7y80LoI5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5c40c07f-4fab-4ffb-d00b-611e6f3c24f2"
      },
      "source": [
        "train = data[0:25600]\n",
        "validation = data[25600:28800]\n",
        "test = data[28800:]\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>emojis</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>normalized_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>328</td>\n",
              "      <td>not yet unfortunately another few weeks i ve been told how are you i ve been a little bit busy with my latest project</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[not, yet, unfortunately, another, few, week, i, ve, been, told, how, are, you, i, ve, been, a, little, bit, busy, with, my, latest, project]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>736</td>\n",
              "      <td>still doing my homework</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[still, doing, my, homework]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1587841</td>\n",
              "      <td>you got an course like that in your degree never heard of it</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "      <td>[you, got, an, course, like, that, in, your, degree, never, heard, of, it]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1592409</td>\n",
              "      <td>you stupid girl hahah do not be negative i want it too but i have to wait we can be tortured together okay</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "      <td>[you, stupid, girl, hahah, do, not, be, negative, i, want, it, too, but, i, have, to, wait, we, can, be, tortured, together, okay]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6543</td>\n",
              "      <td>more snow this morning here in the adirondacks it s covering my crocus that were coming up</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[more, snow, this, morning, here, in, the, adirondacks, it, s, covering, my, crocus, that, were, coming, up]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                                                                                                               normalized_tweet\n",
              "0  328         ...  [not, yet, unfortunately, another, few, week, i, ve, been, told, how, are, you, i, ve, been, a, little, bit, busy, with, my, latest, project]\n",
              "1  736         ...  [still, doing, my, homework]                                                                                                                 \n",
              "2  1587841     ...  [you, got, an, course, like, that, in, your, degree, never, heard, of, it]                                                                   \n",
              "3  1592409     ...  [you, stupid, girl, hahah, do, not, be, negative, i, want, it, too, but, i, have, to, wait, we, can, be, tortured, together, okay]           \n",
              "4  6543        ...  [more, snow, this, morning, here, in, the, adirondacks, it, s, covering, my, crocus, that, were, coming, up]                                 \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rLGLjTYoJU2"
      },
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models.doc2vec import TaggedDocument"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_XFf3Ivocz6"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens\n",
        "train_tagged = train.apply(\n",
        "    lambda r: TaggedDocument(words=r['normalized_tweet'], tags=r['sentiment']), axis=1)\n",
        "test_tagged = test.apply(\n",
        "    lambda r: TaggedDocument(words=r['normalized_tweet'], tags=r['sentiment']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOmvqSfJpeyI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fde170fe-d39e-4e9a-b3a6-ed661ba520aa"
      },
      "source": [
        "train_tagged.values[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaggedDocument(words=['i', 'guess', 'i', 'm', 'out', 'of', 'funny'], tags='0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG4e1mKap4AG"
      },
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn9cKJjCqwl4"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSREew3vp726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e13e09ba-c46f-4c11-d422-7cb3548e76fa"
      },
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25600/25600 [00:00<00:00, 2170052.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cVtM6vnqPZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "a3dd9ea4-8793-456e-9a75-6730973e3c11"
      },
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25600/25600 [00:00<00:00, 1315344.26it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2675525.33it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2040791.09it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2719986.38it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2321202.44it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2741725.16it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2198218.53it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2787274.68it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2794674.33it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2804968.19it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2884929.27it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 1982207.21it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2819107.92it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2579312.08it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2858128.79it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2673992.84it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2018311.70it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2829732.04it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2796129.85it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2857216.14it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2729944.64it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 1207971.63it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 1984368.55it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2812535.88it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 1793455.53it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2809371.60it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2028415.65it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2777253.70it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2792566.51it/s]\n",
            "100%|██████████| 25600/25600 [00:00<00:00, 2696014.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 7s, sys: 7.95 s, total: 1min 14s\n",
            "Wall time: 46.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yvtYiOxrZ0z"
      },
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWniFbmbriEF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "70b4411d-eec3-4e58-a949-4cb949dab948"
      },
      "source": [
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy 0.5678125\n",
            "Testing F1 score: 0.5677840104518899\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}